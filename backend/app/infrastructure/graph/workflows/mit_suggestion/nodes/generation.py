"""ìƒˆ Decision ìƒì„± ë…¸ë“œ"""

import json
import logging
from typing import Any

from langchain_core.prompts import ChatPromptTemplate

from app.infrastructure.graph.integration.llm import get_decision_generator_llm
from app.infrastructure.graph.workflows.mit_suggestion.state import (
    MitSuggestionState,
)
from app.prompt.v1.workflows.mit_suggestion import (
    CONFIDENCE_LEVELS,
    DECISION_GENERATION_HUMAN,
    DECISION_GENERATION_SYSTEM,
    DEFAULT_CONFIDENCE,
)

logger = logging.getLogger(__name__)


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (app.prompt.v1.workflows.mit_suggestionì—ì„œ import)
DECISION_GENERATION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", DECISION_GENERATION_SYSTEM),
    ("human", DECISION_GENERATION_HUMAN),
])


def _format_span_ref(span: dict[str, Any]) -> str:
    """SpanRef dictë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë‹¨ì¼ ë¼ì¸ìœ¼ë¡œ í¬ë§·í•œë‹¤. (fallbackìš©)"""
    transcript_id = span.get("transcript_id", "meeting-transcript")
    start_utt = span.get("start_utt_id")
    end_utt = span.get("end_utt_id")
    if start_utt and end_utt:
        utt_range = f"{start_utt}~{end_utt}"
    else:
        utt_range = start_utt or end_utt or "unknown"

    parts = [f"{transcript_id}:{utt_range}"]
    if span.get("topic_name"):
        parts.append(f"topic={span['topic_name']}")
    if span.get("start_ms") is not None and span.get("end_ms") is not None:
        parts.append(f"{span['start_ms']}ms~{span['end_ms']}ms")
    return " | ".join(parts)


def _format_evidence_text(evidence_text: dict[str, Any]) -> str:
    """evidence_text dictë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·í•œë‹¤.

    í¬ë§·:
        ğŸ“ ê·¼ê±° (2:30~3:20) [í† í”½ëª…]
        (ì• ì»¨í…ìŠ¤íŠ¸)
        - ê¹€ì² ìˆ˜: ë°œí™” ë‚´ìš©...
        â–¶ ê·¼ê±° ë°œí™”
        - ë°•ë¯¼ìˆ˜: í•µì‹¬ ë°œí™” ë‚´ìš©...
        (ë’¤ ì»¨í…ìŠ¤íŠ¸)
        - ì´ì˜í¬: ë°œí™” ë‚´ìš©...
    """
    lines: list[str] = []

    # í—¤ë”
    time_range = evidence_text.get("time_range", "")
    topic_name = evidence_text.get("topic_name")
    header = f"ğŸ“ ê·¼ê±° ({time_range})"
    if topic_name:
        header += f" [{topic_name}]"
    lines.append(header)

    # ì• ì»¨í…ìŠ¤íŠ¸
    before_context = evidence_text.get("before_context", [])
    if before_context:
        lines.append("  (ì• ì»¨í…ìŠ¤íŠ¸)")
        for utt in before_context[-3:]:  # ìµœëŒ€ 3ê°œ
            speaker = utt.get("speaker", "Unknown")
            text = utt.get("text", "")[:80]
            lines.append(f"  - {speaker}: {text}")

    # ê·¼ê±° ë°œí™”
    evidence_utterances = evidence_text.get("evidence_utterances", [])
    if evidence_utterances:
        lines.append("  â–¶ ê·¼ê±° ë°œí™”")
        for utt in evidence_utterances[:5]:  # ìµœëŒ€ 5ê°œ
            speaker = utt.get("speaker", "Unknown")
            text = utt.get("text", "")[:150]
            lines.append(f"  - {speaker}: {text}")

    # ë’¤ ì»¨í…ìŠ¤íŠ¸
    after_context = evidence_text.get("after_context", [])
    if after_context:
        lines.append("  (ë’¤ ì»¨í…ìŠ¤íŠ¸)")
        for utt in after_context[:3]:  # ìµœëŒ€ 3ê°œ
            speaker = utt.get("speaker", "Unknown")
            text = utt.get("text", "")[:80]
            lines.append(f"  - {speaker}: {text}")

    return "\n".join(lines)


async def generate_new_decision(state: MitSuggestionState) -> dict:
    """Suggestionì„ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ Decision ë‚´ìš© ìƒì„±

    Contract:
        reads: mit_suggestion_content, mit_suggestion_decision_content,
               mit_suggestion_decision_context, mit_suggestion_agenda_topic,
               mit_suggestion_gathered_context
        writes: mit_suggestion_new_decision_content, mit_suggestion_supersedes_reason,
                mit_suggestion_confidence
        side-effects: LLM API í˜¸ì¶œ
        failures: GENERATION_FAILED -> ì›ë³¸ Decision ë‚´ìš© ìœ ì§€ + low confidence
    """
    suggestion_content = state.get("mit_suggestion_content", "")
    decision_content = state.get("mit_suggestion_decision_content", "")
    decision_context = state.get("mit_suggestion_decision_context") or ""
    agenda_topic = state.get("mit_suggestion_agenda_topic") or "ì•ˆê±´ ì •ë³´ ì—†ìŒ"
    gathered_context = state.get("mit_suggestion_gathered_context") or {}
    if gathered_context.get("agenda_topic"):
        agenda_topic = gathered_context.get("agenda_topic")

    # íšŒì˜ ì •ë³´ ì„¹ì…˜ êµ¬ì„±
    meeting_section = ""
    if gathered_context.get("meeting_context"):
        mc = gathered_context["meeting_context"]
        meeting_parts = []
        if mc.get("meeting_title"):
            meeting_parts.append(f"- íšŒì˜ ì œëª©: {mc['meeting_title']}")
        if mc.get("meeting_date"):
            meeting_parts.append(f"- íšŒì˜ ë‚ ì§œ: {mc['meeting_date']}")
        if mc.get("agenda_topics"):
            topics = ", ".join(mc["agenda_topics"][:5])  # ìµœëŒ€ 5ê°œ
            meeting_parts.append(f"- ì „ì²´ ì•ˆê±´: {topics}")
        if meeting_parts:
            meeting_section = "[íšŒì˜ ì •ë³´]\n" + "\n".join(meeting_parts)

    # ê¸°ì¡´ ë…¼ì˜ ë‚´ìš© ì„¹ì…˜ êµ¬ì„±
    thread_section = ""
    if gathered_context.get("thread_history"):
        thread_items = []
        for h in gathered_context["thread_history"][-5:]:  # ìµœê·¼ 5ê°œ
            author = h.get("author", "Unknown")
            content = h.get("content", "")[:100]
            thread_items.append(f"- {author}: {content}...")
        if thread_items:
            thread_section = "[ê¸°ì¡´ ë…¼ì˜ ë‚´ìš©]\n" + "\n".join(thread_items)

    # ê´€ë ¨ ê²°ì •ì‚¬í•­ ì„¹ì…˜ êµ¬ì„±
    sibling_section = ""
    if gathered_context.get("sibling_decisions"):
        sibling_items = []
        for d in gathered_context["sibling_decisions"][:3]:  # ìµœëŒ€ 3ê°œ
            content = d.get("content", "")[:100]
            status = d.get("status", "unknown")
            sibling_items.append(f"- [{status}] {content}...")
        if sibling_items:
            sibling_section = "[ê´€ë ¨ ê²°ì •ì‚¬í•­ (ê°™ì€ ì•ˆê±´)]\n" + "\n".join(sibling_items)

    # ê·¼ê±° ì„¹ì…˜ êµ¬ì„± (ì‹¤ì œ í…ìŠ¤íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ SpanRef ë©”íƒ€ë°ì´í„° fallback)
    evidence_parts: list[str] = []

    # 1. ì›ë³¸ ê²°ì •ì‚¬í•­ ê·¼ê±°
    decision_evidence_texts = gathered_context.get("decision_evidence_texts") or []
    decision_evidence = gathered_context.get("decision_evidence") or []

    if decision_evidence_texts:
        # ì‹¤ì œ ë°œí™” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
        evidence_parts.append("[ì›ë³¸ ê²°ì •ì‚¬í•­ ê·¼ê±°]")
        for et in decision_evidence_texts[:3]:  # ìµœëŒ€ 3ê°œ
            evidence_parts.append(_format_evidence_text(et))
    elif decision_evidence:
        # fallback: SpanRef ë©”íƒ€ë°ì´í„°ë§Œ
        evidence_parts.append("[ì›ë³¸ ê²°ì •ì‚¬í•­ ê·¼ê±° SpanRef]")
        evidence_parts.extend(
            f"- {_format_span_ref(span)}" for span in decision_evidence[:8]
        )

    # 2. ì•ˆê±´ ê·¼ê±°
    agenda_evidence_texts = gathered_context.get("agenda_evidence_texts") or []
    agenda_evidence = gathered_context.get("agenda_evidence") or []

    if agenda_evidence_texts:
        evidence_parts.append("\n[ì•ˆê±´ ê·¼ê±°]")
        for et in agenda_evidence_texts[:2]:  # ìµœëŒ€ 2ê°œ
            evidence_parts.append(_format_evidence_text(et))
    elif agenda_evidence:
        evidence_parts.append("[ì•ˆê±´ ê·¼ê±° SpanRef]")
        evidence_parts.extend(
            f"- {_format_span_ref(span)}" for span in agenda_evidence[:5]
        )

    # 3. ê´€ë ¨ ê²°ì •ì‚¬í•­ ê·¼ê±° (SpanRefë§Œ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¯¸ì§€ì›)
    sibling_evidence_lines: list[str] = []
    for sibling in (gathered_context.get("sibling_decisions") or [])[:3]:
        sibling_evidence = sibling.get("evidence") or []
        if not sibling_evidence:
            continue
        sibling_content = sibling.get("content", "")[:60]
        sibling_status = sibling.get("status", "unknown")
        sibling_evidence_lines.append(f"- [{sibling_status}] {sibling_content}...")
        sibling_evidence_lines.extend(
            f"  - {_format_span_ref(span)}" for span in sibling_evidence[:2]
        )
    if sibling_evidence_lines:
        evidence_parts.append("\n[ê´€ë ¨ ê²°ì •ì‚¬í•­ ê·¼ê±°]")
        evidence_parts.extend(sibling_evidence_lines)

    evidence_section = (
        "\n".join(evidence_parts)
        if evidence_parts
        else "[ê·¼ê±°]\nì œê³µëœ ê·¼ê±° ì—†ìŒ"
    )

    try:
        llm = get_decision_generator_llm()
        chain = DECISION_GENERATION_PROMPT | llm

        result = await chain.ainvoke({
            "meeting_section": meeting_section if meeting_section else "[íšŒì˜ ì •ë³´]\nì •ë³´ ì—†ìŒ",
            "agenda_topic": agenda_topic,
            "decision_content": decision_content,
            "decision_context": decision_context if decision_context else "ë§¥ë½ ì •ë³´ ì—†ìŒ",
            "evidence_section": evidence_section,
            "thread_section": thread_section if thread_section else "[ê¸°ì¡´ ë…¼ì˜ ë‚´ìš©]\në…¼ì˜ ë‚´ì—­ ì—†ìŒ",
            "sibling_section": sibling_section if sibling_section else "",
            "suggestion_content": suggestion_content,
        })

        response_text = result.content if hasattr(result, 'content') else str(result)

        # JSON íŒŒì‹± ì‹œë„
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” ìˆœìˆ˜ JSON)
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0].strip()
            else:
                json_str = response_text.strip()

            parsed = json.loads(json_str)

            new_content = parsed.get("new_decision_content")
            reason = parsed.get("supersedes_reason")
            confidence = parsed.get("confidence")

            # H2: Validate and log when fallback values are used
            if not new_content:
                logger.warning(
                    "[generate_new_decision] Missing new_decision_content in response, "
                    "using original decision content as fallback"
                )
                new_content = decision_content
            if not reason:
                logger.info("[generate_new_decision] Missing supersedes_reason, using default")
                reason = "ì‚¬ìš©ì ì œì•ˆ ë°˜ì˜"
            if not confidence:
                logger.info("[generate_new_decision] Missing confidence, using default")
                confidence = DEFAULT_CONFIDENCE

            # confidence ê°’ ê²€ì¦
            if confidence not in CONFIDENCE_LEVELS:
                confidence = DEFAULT_CONFIDENCE

            logger.info(
                f"[generate_new_decision] Generated: "
                f"confidence={confidence}, content_len={len(new_content)}"
            )

            return {
                "mit_suggestion_new_decision_content": new_content,
                "mit_suggestion_supersedes_reason": reason,
                "mit_suggestion_confidence": confidence,
            }

        except json.JSONDecodeError as e:
            logger.warning(f"[generate_new_decision] JSON parse failed: {e}")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‘ë‹µì„ Decisionìœ¼ë¡œ ì‚¬ìš©
            return {
                "mit_suggestion_new_decision_content": response_text[:500],
                "mit_suggestion_supersedes_reason": "ì‚¬ìš©ì ì œì•ˆ ë°˜ì˜ (AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)",
                "mit_suggestion_confidence": "low",
            }

    except Exception:
        logger.exception("[generate_new_decision] LLM call failed")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì›ë³¸ Decision ìœ ì§€
        return {
            "mit_suggestion_new_decision_content": decision_content,
            "mit_suggestion_supersedes_reason": "AI ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "mit_suggestion_confidence": "low",
        }
