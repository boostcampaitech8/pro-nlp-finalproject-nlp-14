# 프로덕션 환경 - 선언형 기본값
# - 비밀값: External Secret(ESO + GCP Secret Manager)로 mit-secrets 관리
# - 비비밀값: 이 파일과 k8s/image-tags.yaml에서 관리

global:
  namespace: mit
  environment: production
  registry: "ghcr.io/teamatoi"

# HA 설정 (3노드 클러스터)
# backend: context_runtime이 인메모리 캐시(TTLCache)라 멀티 인스턴스 시 토픽 정합성 깨짐
# TODO: context_runtime을 Redis 기반으로 전환 후 replicas 증가
replicas:
  backend: 1
  frontend: 2
  arqWorker: 2

# 이미지 태그는 k8s/image-tags.yaml에서 override
images:
  backend:
    tag: "latest"
  frontend:
    tag: "latest"
  worker:
    tag: "latest"
  pullSecret: "ghcr-secret"  # GHCR Private 레지스트리 인증용

# 앱 환경변수 Secret 참조 이름 (Secret 본문은 외부에서 관리)
secrets:
  name: "mit-secrets"

# Ingress (Traefik + Let's Encrypt)
ingress:
  className: traefik
  host: "www.mit-hub.com"
  tls: true
  certManager: true
  clusterIssuer: "letsencrypt-prod"

# 앱 포트
backend:
  port: 8000

frontend:
  port: 80

# Redis (standalone - 앱 레벨 Sentinel 미지원으로 단일 인스턴스 유지)
redis:
  host: redis-master
  port: 6379

# LiveKit (서비스 이름 lk-server: LIVEKIT_PORT 환경변수 충돌 방지)
# 포트 80: Helm 차트가 서비스 80 -> 컨테이너 7880 매핑
livekit:
  host: lk-server
  port: 80
  # Webhook 검증용 key (Secret의 keys.yaml key와 동일해야 함)
  apiKey: "mit-api-key"
  externalUrl: "wss://www.mit-hub.com/livekit"
  webhookUrls:
    - "http://backend:8000/api/v1/livekit/webhook"
  # 노드 1 고정 (WebRTC는 외부 IP가 고정되어야 함)
  # 배포 후: kubectl label node <노드1> mit/livekit=true
  nodeSelector:
    mit/livekit: "true"
  # RTC 설정 (프로덕션: LB 공인 IP를 ICE candidate로 광고)
  rtc:
    nodeIp: ""  # TODO: 실제 LB 공인 IP를 고정값으로 입력
    useExternalIp: false
    tcpPort: 7881
    portRangeStart: 50000
    portRangeEnd: 51000

# JWT
jwt:
  accessTokenExpireMinutes: 30
  refreshTokenExpireDays: 7

# App
app:
  debug: false
  logLevel: INFO
  corsOrigins:
    - "https://mit-hub.com"
    - "https://www.mit-hub.com"
  frontendBaseUrl: "https://www.mit-hub.com"  # 초대 링크 생성용

# Worker
worker:
  clovaSttEndpoint: "clovaspeech-gw.ncloud.com:50051"
  ttsVolumeScale: "0.70"

# Resources
resources:
  backend:
    requests: { memory: "512Mi", cpu: "200m" }
    limits:   { memory: "1Gi",   cpu: "500m" }
  frontend:
    requests: { memory: "32Mi",  cpu: "50m" }
    limits:   { memory: "128Mi", cpu: "200m" }
  worker:
    requests: { memory: "128Mi", cpu: "200m" }
    limits:   { memory: "256Mi", cpu: "500m" }
  arqWorker:
    requests: { memory: "256Mi", cpu: "100m" }
    limits:   { memory: "512Mi", cpu: "500m" }
  redis:
    requests: { memory: "128Mi", cpu: "50m" }
    limits:   { memory: "256Mi", cpu: "200m" }
    storage: 1Gi
  livekit:
    requests: { memory: "512Mi", cpu: "500m" }
    limits:   { memory: "1Gi",   cpu: "1000m" }
  # Observability 스택
  prometheus:
    requests: { memory: "512Mi", cpu: "200m" }
    limits: { memory: "1Gi", cpu: "500m" }
  loki:
    requests: { memory: "256Mi", cpu: "100m" }
    limits: { memory: "512Mi", cpu: "300m" }
  alloy:
    requests: { memory: "128Mi", cpu: "50m" }
    limits: { memory: "256Mi", cpu: "200m" }
  grafana:
    requests: { memory: "128Mi", cpu: "50m" }
    limits: { memory: "256Mi", cpu: "200m" }

# Observability 설정
observability:
  prometheus:
    storage: 10Gi
  loki:
    storage: 10Gi
  grafana:
    storage: 5Gi

# Redis 노드 고정 (노드1: LiveKit과 동일 노드)
# 배포 후: kubectl label node <노드1> mit/redis=true
redisNodeSelector:
  mit/redis: "true"

# Observability 스택 노드 고정 (local-path PV는 노드 로컬 디스크 사용)
# Pod이 다른 노드로 재스케줄되면 기존 PV 데이터에 접근 불가 -> 같은 노드 고정
# 배포 후: kubectl label node <노드2 또는 노드3> mit/stateful=true
statefulNodeSelector:
  mit/stateful: "true"

# Cloudflare Tunnel (Grafana 접근용)
cloudflare:
  enabled: true
